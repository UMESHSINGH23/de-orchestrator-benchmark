# de-orchestrator-benchmark
A reproducible Data Engineering project benchmarking Apache Airflow, Luigi, and Prefect for tabular ETL pipelines using a controlled experimental methodology.
# Data Engineering Project – Technical Submission (Part 2)

## Team & Roles

**Group:** WS25–DEXX

**Student A – Technical Lead**
- Full technical pipeline implementation
- Airflow DAG design and execution
- Figures and tables generation from code
- Ensuring reproducibility and correct file naming

**Student B – Documentation & Presentation Lead**
- Final report writing (Overleaf)
- Presentation slides preparation
- Formatting, clarity, and consistency of results
# End-to-End Data Pipeline with Apache Airflow

## Project Overview
This project demonstrates an end-to-end data pipeline built using Apache Airflow. 
The pipeline covers data ingestion, cleaning, feature engineering, model training, and evaluation.

The goal is to showcase how real-world data science projects are structured and automated.

---

## Project Structure
de-orchestrator-benchmark/
<br>
├── dags/                      # Airflow DAGs
<br>
├── src/                       # Core business logic
<br>
│   ├── data_ingestion/
<br>
│   ├── data_cleaning/
<br>
│   ├── feature_engineering/
<br>
│   ├── modeling/
<br>
│   └── evaluation/
<br>
├── data/                      # Datasets
<br>
├── figures/                   # Plots & charts
<br>
├── tables/                    # Result tables
<br>
├── requirements.txt
<br>
└── README.md
<br>
---

## ⚙️ Technologies Used
- Python
- Apache Airflow
- Pandas
- NumPy
- Scikit-learn

---

## Pipeline Flow
1. Data Ingestion
2. Data Cleaning
3. Feature Engineering
4. Model Training
5. Model Evaluation

---

## Use Case
This structure reflects how data pipelines are implemented in:
- FinTech
- HealthTech
- E-commerce
- Risk & Credit Scoring Systems

---

## Future Improvements
- Add data validation
- Add logging & monitoring
- Add CI/CD
- Deploy using Docker

---

## Author
Umesh Singh  
MSc Data Science | Germany
